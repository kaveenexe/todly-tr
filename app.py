# app.py
from flask import Flask, request, jsonify
from transformers import MT5ForConditionalGeneration, T5Tokenizer
import torch
from google.cloud import translate_v2 as translate
import requests
import os

# initialize flask app
app = Flask(__name__)

# Set environment variable for Google Cloud credentials
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "credentials/google_api_key.json"

# Path to the directory containing the downloaded files
model_dir = "Model"

# Load tokenizer with explicit legacy setting
tokenizer = T5Tokenizer.from_pretrained(model_dir, legacy=True)

# Load model correctly as MT5ForConditionalGeneration
model = MT5ForConditionalGeneration.from_pretrained(model_dir)

# Initialize Google Translate API client
translate_client = translate.Client()

def translate_with_confidence(text, source_lang, target_lang):
    input_text = f"{source_lang} to {target_lang}: {text}"
    input_ids = tokenizer.encode(input_text, return_tensors="pt") # Tokenize
    outputs = model.generate(
        input_ids=input_ids,
        max_length=256,
        num_beams=4,
        early_stopping=True,
        return_dict_in_generate=True,
        output_scores=True
    )
    translated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True,
                                       clean_up_tokenization_spaces=True)
    beam_scores = outputs.sequences_scores # sequences generated by the model
    confidence = -beam_scores[0].item()

    logits = outputs.scores
    tokens = outputs.sequences[0]

    confidence_scores = []
    for logit, token_id in zip(logits, tokens[1:]):
        probabilities = torch.softmax(logit, dim=-1)
        if token_id < probabilities.shape[-1]:
            confidence_scores.append(probabilities[0, token_id].item())

    average_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0

    return translated_text, confidence, average_confidence

def translate_with_api(text, source_lang, target_lang):
    result = translate_client.translate(
        text,
        source_language=source_lang,
        target_language=target_lang
    )
    return result['translatedText']

@app.route('/')
def home():
    return "Welcome to the Toddler App API! Use the /process endpoint to interact with the service."

@app.route('/process', methods=['POST'])
def process_request():
    data = request.json
    question = data.get("question")
    source_lang = data.get("source_lang", "si")
    target_lang = data.get("target_lang", "en")

    # Translate question from Sinhala to English
    translated_question, beam_confidence, avg_token_confidence = translate_with_confidence(
        question, source_lang, target_lang
    )

    # Check confidence and decide on using fallback API if confidence is low
    if beam_confidence < 0.5:
        translated_question = translate_with_api(question, source_lang, target_lang)

    # Send translated question to the LLM with "stream": false to get a full response
    llm_response = requests.post(
        "http://34.67.106.101:11434//api/generate",
        headers={"Content-Type": "application/json"},
        json={"model": "blackalpha/todlymist", "prompt": translated_question, "stream": False}  # Added "stream": False
    )

    # Log the full response from LLM for debugging
    print("LLM Response Status:", llm_response.status_code)
    print("LLM Response Content:", llm_response.text)

    if llm_response.status_code == 200:
        # Assuming the full response is returned in 'response'
        english_answer = llm_response.json().get("response")
    else:
        return jsonify({"error": "Failed to get a response from LLM"}), 500

    # Translate the English answer back to Sinhala
    final_answer, _, _ = translate_with_confidence(english_answer, "en", "si")

    return jsonify({"answer": final_answer})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
